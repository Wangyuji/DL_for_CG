{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3d4156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c666c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "Load data from tictac_final\n",
      "(958, 9)\n",
      "(958, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#file_path = 'tic_dataset/tictac_final.txt'\n",
    "file_path = 'D:\\\\UFL\\\\cap5404\\\\Part1\\\\tictactoedatasets\\\\tictac_final.txt'\n",
    "print('File exists:', os.path.exists(file_path))\n",
    "print(\"Load data from tictac_final\")\n",
    "np.arr = np.loadtxt(file_path)\n",
    "\n",
    "X = np.arr[:, : 9 ]\n",
    "Y = np.arr[:, 9 : ]\n",
    "    \n",
    "#Y = Y.reshape((958,9))\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1121f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 9) (191, 1)\n",
      "(767, 9) (767, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.8)\n",
    "print (x_train.shape, y_train.shape)\n",
    "print (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe5a1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_MLP():\n",
    "    param_setting =[{\n",
    "        'hidden_layer_sizes': [(150,100,50,10)],\n",
    "        'activation':['relu', 'sigmoid'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant']\n",
    "    }]\n",
    "    \n",
    "    kfd = KFold(n_splits = 20, shuffle=True, random_state = 10)\n",
    "    \n",
    "    iter_clf = MLPClassifier(max_iter = 1000)\n",
    "    #class sklearn.model_selection.GridSearchCV(estimator, param_grid, *, \n",
    "    #scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, \n",
    "    #return_train_score=False)[source]\n",
    "    clf = GridSearchCV(iter_clf, param_setting, n_jobs =-1, cv=kfd, scoring=\"accuracy\")\n",
    "    \n",
    "    iter_reg = MLPRegressor(random_state=1, max_iter=1000)\n",
    "    \n",
    "    reg = GridSearchCV(iter_reg, param_setting, cv=kfd, n_jobs=-1)\n",
    "    \n",
    "    return clf, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1fcc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_classifier(x_train, y_train, x_test, y_test):\n",
    "    clf, reg = _init_MLP()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    best_score = clf.best_score_\n",
    "    best_params = clf.best_params_\n",
    "    best_esti = clf.best_estimator_\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    print(\"The best socre is: \")\n",
    "    print(best_score)\n",
    "    print(\"The best params is: \")\n",
    "    print(best_params)\n",
    "    #print(\"The best estimator is: \")\n",
    "    #print(best_esti)\n",
    "    #print(\"The best prediction for y is: \")\n",
    "    #print(y_pred)\n",
    "    \n",
    "    #nomalize default is true here\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    print('Test accuracy:', test_acc)\n",
    "    test_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    print('Test accuracy with normalize flase: \\n', test_acc)  \n",
    "\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = cm/cm.astype(np.float).sum(axis = 1)\n",
    "    print(\"Confusion Matrix is: \")\n",
    "    print(cm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f486dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "40 fits failed out of a total of 80.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 385, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 503, in _validate_hyperparameters\n",
      "    raise ValueError(\n",
      "ValueError: The activation 'sigmoid' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.93611111 0.93666667        nan        nan]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1118: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best socre is: \n",
      "0.9366666666666668\n",
      "The best params is: \n",
      "{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (150, 100, 50, 10), 'learning_rate': 'constant'}\n",
      "Test accuracy: 0.9674054758800521\n",
      "Test accuracy with normalize flase: \n",
      " 742\n",
      "Confusion Matrix is: \n",
      "[[0.91078067 0.04819277]\n",
      " [0.00371747 0.99799197]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14044\\AppData\\Local\\Temp\\ipykernel_10668\\2775301289.py:27: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cm = cm/cm.astype(np.float).sum(axis = 1)\n"
     ]
    }
   ],
   "source": [
    "MLP_classifier(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "268e0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_regressor(x_train, y_train, x_test, y_test):\n",
    "    clf, reg = _init_MLP()\n",
    "    reg.fit(x_train, y_train)\n",
    "    \n",
    "     \n",
    "    best_score = reg.best_score_\n",
    "    best_params = reg.best_params_\n",
    "    best_esti = reg.best_estimator_\n",
    "    y_pred = reg.predict(x_test)\n",
    "    \n",
    "    print(\"The best socre is: \")\n",
    "    print(best_score)\n",
    "    print(\"The best params is: \")\n",
    "    print(best_params)\n",
    "    \n",
    "    #y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "    #nomalize default is true here\n",
    "    #test_acc = np.arange(9)\n",
    "    #for i in range(9):\n",
    "       # test_acc[i] = accuracy_score(y_test[:,i], y_pred[i])\n",
    "       # total += test[i]\n",
    "        \n",
    "   # final_total = total/(np.shape(y_test)[0] * 9)  \n",
    " #   print('Test accuracy:', final_total)\n",
    "    #test_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    #print('Test accuracy with normalize flase: \\n', test_acc)  \n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = cm/cm.astype(np.float).sum(axis = 1)\n",
    "    print(\"Confusion Matrix is: \")\n",
    "    print(cm)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae3bdc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "40 fits failed out of a total of 80.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 385, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 503, in _validate_hyperparameters\n",
      "    raise ValueError(\n",
      "ValueError: The activation 'sigmoid' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.73028888 0.73440304        nan        nan]\n",
      "  warnings.warn(\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1607: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best socre is: \n",
      "0.7344030426567237\n",
      "The best params is: \n",
      "{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (150, 100, 50, 10), 'learning_rate': 'constant'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mMLP_regressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36mMLP_regressor\u001b[1;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_params)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#y_pred = np.where(y_pred > 0.5, 1, 0)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#nomalize default is true here\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#test_acc = np.arange(9)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#test_acc = accuracy_score(y_test, y_pred, normalize=False)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#print('Test accuracy with normalize flase: \\n', test_acc)  \u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m cm \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m/\u001b[39mcm\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum(axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix is: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusion_matrix\u001b[39m(\n\u001b[0;32m    223\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    224\u001b[0m ):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "MLP_regressor(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fa284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
